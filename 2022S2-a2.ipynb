{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Naive Bayes [20 marks]\n",
    "\n",
    "Student Name: Cheah Jia Huei\n",
    "\n",
    "Student ID: 1078203\n",
    "\n",
    "## General info\n",
    "\n",
    "<b>Due date</b>: Friday, 2 September 2022, 5pm\n",
    "\n",
    "<b>Submission method</b>: Canvas submission\n",
    "\n",
    "<b>Submission materials</b>: completed copy of this iPython notebook\n",
    "\n",
    "<b>Late submissions</b>: -10% per day up to 5 days (both weekdays and weekends count)\n",
    "<ul>\n",
    "    <li>one day late, -2.0;</li>\n",
    "    <li>two days late, -4.0;</li>\n",
    "    <li>three days late, -6.0;</li>\n",
    "    <li>four days late, -8.0;</li>\n",
    "    <li>five days late, -10.0;</li>\n",
    "</ul>\n",
    "\n",
    "<b>Marks</b>: 20% of mark for class. \n",
    "\n",
    "<b>Materials</b>: See [Using Jupyter Notebook and Python page](https://canvas.lms.unimelb.edu.au/courses/126693/pages/python-and-jupyter-notebooks?module_item_id=3950453) on Canvas (under Modules> Coding Resources) for information on the basic setup required for this class, including an iPython notebook viewer.If your iPython notebook doesn't run on the marker's machine, you will lose marks. <b> You should use Python 3</b>.  \n",
    "\n",
    "\n",
    "<b>Evaluation</b>: Your iPython notebook should run end-to-end without any errors in a reasonable amount of time, and you must follow all instructions provided below, including specific implementation requirements and instructions for what needs to be printed (please avoid printing output we don't ask for). You should implement functions for the skeletons listed below. You may implement any number of additional (helper) functions. You should leave the output from running your code in the iPython notebook you submit, to assist with marking. The amount each section is worth is given in parenthesis after the instructions. \n",
    "\n",
    "You will be marked not only on the correctness of your methods, but also the quality and efficiency of your code: in particular, you should be careful to use Python built-in functions and operators when appropriate and pick descriptive variable names that adhere to <a href=\"https://www.python.org/dev/peps/pep-0008/\">Python style requirements</a>. If you think it might be unclear what you are doing, you should comment your code to help the marker make sense of it. We reserve the right to deduct up to 4 marks for unreadable or excessively inefficient code.\n",
    "\n",
    "7 of the marks available for this Project will be assigned to whether the five specified Python functions work in a manner consistent with the materials from COMP90049. Any other implementation will not be directly assessed (except insofar as it is required to make these five functions work correctly).\n",
    "\n",
    "13 of the marks will be assigned to your responses to the questions, in terms of both accuracy and insightfulness. We will be looking for evidence that you have an implementation that allows you to explore the problem, but also that you have thought deeply about the data and the behaviour of the Naive Bayes classifier.\n",
    "\n",
    "<b>Updates</b>: Any major changes to the assignment will be announced via Canvas. Minor changes and clarifications will be announced on the discussion board (Piazza -> Assignments -> A2); we recommend you check it regularly.\n",
    "\n",
    "<b>Academic misconduct</b>: While you may discuss this homework in general terms with other students, it ultimately is still an individual task. Reuse of code or other instances of clear influence will be considered cheating. Please check the <a href=\"https://canvas.lms.unimelb.edu.au/courses/126693/modules#module_734188\">CIS Academic Honesty training</a> for more information. We will be checking submissions for originality and will invoke the University’s <a href=\"http://academichonesty.unimelb.edu.au/policy.html\">Academic Misconduct policy</a> where inappropriate levels of collusion or plagiarism are deemed to have taken place.\n",
    "\n",
    "**IMPORTANT**\n",
    "\n",
    "Please carefully read and fill out the <b>Authorship Declaration</b> form at the bottom of the page. Failure to fill out this form results in the following deductions: \n",
    "<UL TYPE=”square”>\n",
    "<LI>missing Authorship Declaration at the bottom of the page, -10.0\n",
    "<LI>incomplete or unsigned Authorship Declaration at the bottom of the page, -5.0\n",
    "</UL>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Base code [7 marks]\n",
    "\n",
    "Instructions\n",
    "1. Do **not** shuffle the data set\n",
    "2. Treat the features as nominal and use them as provided (e.g., do **not** convert them to other feature types, such as numeric ones). Implement a Naive Bayes classifier with appropriate likelihood function for the data.\n",
    "3. You should implement the Naive Bayes classifier from scratch. Do **not** use existing implementations/learning algorithms. You must use epsilon smoothing strategy as discussed in the Naive Bayes lecture. \n",
    "4. Apart from the instructions in point 3, you may use libraries to help you with data reading, representation, maths or evaluation.\n",
    "5. Ensure that all and only required information is printed, as indicated in the final three code cells. Failure to adhere to print the required information will result in **[-1 mark]** per case. *(We don't mind details like you print a list or several numbers -- just make sure the information is displayed so that it's easily accessible)*\n",
    "6. Please place the jupyter notebook into the same folder as the input data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should open a csv file and read the data into a useable format [0.5 mark]\n",
    "\n",
    "def preprocess(filename):\n",
    "    X = list()\n",
    "    label = list()\n",
    "    X_and_label = list()\n",
    "    data = open(filename).readlines()\n",
    "    temp_col = data[0].strip().split(\",\")\n",
    "    feat_headers = temp_col[1:]\n",
    "    for i in range(1,len(data)):\n",
    "        splitted_data = data[i].strip().split(\",\")\n",
    "        X_and_label.append(splitted_data[1:])\n",
    "        X.append(splitted_data[1:-1])\n",
    "        label.append(splitted_data[-1])\n",
    "    return (X, label,feat_headers, X_and_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should build a supervised NB model [3 marks]\n",
    "# the X and label inputs here should belong to the training set\n",
    "import numpy as np\n",
    "import math\n",
    "def train(X,label,X_and_label):\n",
    "    # use epsilon sommothing strategy\n",
    "    feature_vals = list()\n",
    "    # get the set of all feature labels\n",
    "    number_of_features = len(X[0])\n",
    "    for j in range(0,number_of_features):\n",
    "       poss_feat_val = list(set([instance[j] for instance in X]))\n",
    "       feature_vals.append(poss_feat_val)\n",
    "    # get the set of all labels\n",
    "    label_types = list(set(label))\n",
    "    # get the total number of labels\n",
    "    label_total = len(label)\n",
    "    # count the number of occurrences of each label\n",
    "    label_occs = list()\n",
    "    for element in label_types:\n",
    "        label_num = label.count(element)\n",
    "        label_occs.append(label_num)\n",
    "    # convert label_types and their respective occurrences into a dictionary\n",
    "    # this is used in the process of counting P(x|y)\n",
    "    label_count_dict = dict(zip(label_types,label_occs))\n",
    "    label_prob = [occ/label_total for occ in label_occs]\n",
    "    label_prob_dict = dict(zip(label_types,label_prob))\n",
    "    # how to find two element occurring at the same time in a row\n",
    "    # helper function to find P(x|y) of all features\n",
    "    # inner function can access all variables from outer function\n",
    "    def conditional_prob(X,label):\n",
    "        \n",
    "        # create an empty dictionary whih records all of the probability patterns\n",
    "        Y_copy = np.array(X_and_label)\n",
    "        feature_probs = dict()\n",
    "        # outer loop over all label types\n",
    "        for indi_lab in label_types:\n",
    "            # second and third loop for the 2d feature_vals array\n",
    "            for k in range(len(feature_vals)):\n",
    "                for m in range(len(feature_vals[k])):\n",
    "                    result = Y_copy[np.where( (Y_copy[:,k] == feature_vals[k][m]) & (Y_copy[:,-1] == indi_lab))]\n",
    "                    prob_numerator = len(result)\n",
    "                    prob_denomitor = label_count_dict[indi_lab]\n",
    "                    cond_prob = prob_numerator / prob_denomitor\n",
    "                    # choose val of epsiolon: 1 * 10^-5\n",
    "                    epsilon = math.pow(10,-5)\n",
    "                    if cond_prob == 0:\n",
    "                        cond_prob = epsilon\n",
    "                    # The key of the feature_probs dictionary: featureNum_featureVal_label\n",
    "                    fprobs_key = str(k) +'_'+feature_vals[k][m]+'_'+indi_lab\n",
    "                    feature_probs[fprobs_key] = cond_prob\n",
    "        return feature_probs\n",
    "    feature_probs = conditional_prob(X,label)\n",
    "    return (label_prob_dict,feature_probs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should predict the class for a set of instances, based on a trained model [1.5 marks]\n",
    "# label_prob_dict and feature_probs were outputs from the train_model\n",
    "import math\n",
    "def predict(test_instance,label_prob_dict,feature_probs):\n",
    "    label_types = list(label_prob_dict.keys())\n",
    "    # create a probability dict for all outcomes\n",
    "    cat_out = dict()\n",
    "    # to avoid probability underflow, add the log probabilities\n",
    "    # without the base argument in the log function, natural log computed\n",
    "    for indi_label in label_types:\n",
    "        # initialise the probability of that category given the test instance\n",
    "        prob_cat = label_prob_dict[indi_label]\n",
    "        total_cat_prob = math.log(prob_cat)\n",
    "        for feat_num, feat_val in enumerate(test_instance):\n",
    "            request_key = str(feat_num) + \"_\" + feat_val + \"_\" + indi_label\n",
    "            if request_key in feature_probs:           \n",
    "                total_cat_prob += math.log(feature_probs[request_key])\n",
    "            else:\n",
    "                epsilon = math.pow(10,-5)\n",
    "                total_cat_prob += math.log(epsilon)\n",
    "                \n",
    "        cat_out[indi_label] = total_cat_prob\n",
    "    max_key = max(cat_out,key=cat_out.get)\n",
    "    return (cat_out,max_key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should evaluate a set of predictions [1 mark]\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "def evaluate(X,label,label_prob_dict, feature_probs):\n",
    "    # can use sklearn lib\n",
    "    # working on the entire dataset for both train and test \n",
    "    # generate the prediction for the whole dataset \n",
    "    predictions = list()\n",
    "    for i in range(len(X)):\n",
    "        current_prediction = predict(X[i],label_prob_dict,feature_probs)\n",
    "        predictions.append(current_prediction[1])\n",
    "    evaluation_scores = dict()\n",
    "    # get the number of labels\n",
    "    number_of_labels = len(list(set(label)))\n",
    "    # the true labels are 'obese' for obesity dataset and 'yes' for banking dataset\n",
    "    label_types = list(set(label))\n",
    "    if (number_of_labels == 2):\n",
    "        evaluation_scores['f1_score'] = f1_score(label,predictions,pos_label=label_types[0],average='binary')\n",
    "        evaluation_scores['precision'] = precision_score(label,predictions,pos_label=label_types[0],average='binary')\n",
    "        evaluation_scores['recall'] = recall_score(label,predictions,pos_label=label_types[0],average='binary')\n",
    "    elif(number_of_labels > 2):\n",
    "        evaluation_scores['f1_score'] = f1_score(label,predictions,average='macro')\n",
    "        evaluation_scores['precision'] = precision_score(label,predictions,average='macro')\n",
    "        evaluation_scores['recall'] = recall_score(label,predictions,average='macro')\n",
    "    evaluation_scores['accuracy'] = accuracy_score(label,predictions)\n",
    "    evaluation_scores['error_rate'] = 1 - accuracy_score(label,predictions)\n",
    "    return evaluation_scores\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "               \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bank Marketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell should act as your \"main\" function where you call the above functions \n",
    "# on the full Bank Marketing data set, and print the evaluation score. [0.33 marks]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# First, read in the data and apply your NB model to the Bank Marketing data\n",
    "output = preprocess('bank-marketing.csv')\n",
    "X = output[0]\n",
    "# After training the dataset, the label is always added to X, so create a copy using np\n",
    "label = output[1]\n",
    "X_and_label = output[3]\n",
    "train_result = train(X,label,X_and_label)\n",
    "label_prob_dict = train_result[0]\n",
    "feature_probs = train_result[1]\n",
    "evaluation_scores = evaluate(X,label,label_prob_dict,feature_probs)\n",
    "\n",
    "print(evaluation_scores)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Third, print data statistics and model predictions, as instructed below \n",
    "# N is the total number of instances, F the total number of features, L the total number of labels\n",
    "# The \"class probabilities\" may be unnormalized\n",
    "\n",
    "probs_3 = predict(X[-3],label_prob_dict,feature_probs)\n",
    "# round the number of decimals in the \"class probabilities\" dictionary\n",
    "class_prob_dict_3 = probs_3[0]\n",
    "for key_3 in class_prob_dict_3:\n",
    "    class_prob_dict_3[key_3] = round(class_prob_dict_3[key_3],4)\n",
    "\n",
    "probs_2 = predict(X[-2],label_prob_dict,feature_probs)\n",
    "# round the number of decimals in the \"class probabilities\" dictionary\n",
    "class_prob_dict_2 = probs_2[0]\n",
    "for key_2 in class_prob_dict_2:\n",
    "    class_prob_dict_2[key_2] = round(class_prob_dict_2[key_2], 4)\n",
    "\n",
    "probs_1 = predict(X[-1],label_prob_dict,feature_probs)\n",
    "# round the number of decimals in the \"class probabilities\" dictionary\n",
    "class_prob_dict_1 = probs_1[0]\n",
    "for key_1 in class_prob_dict_1:\n",
    "    class_prob_dict_1[key_1] = round(class_prob_dict_1[key_1], 4)\n",
    "print(\"Feature vectors of instances [0, 1, 2]: \", X[0:3])\n",
    "print(\"\\nNumber of instances (N): \", len(X) )\n",
    "print(\"Number of features (F): \", len(X[0]))\n",
    "print(\"Number of labels (L): \", len(set(label)))\n",
    "\n",
    "# should we list out the probability prediction of each class?\n",
    "# N-3 is thrid instance from the end\n",
    "print(\"\\n\\nPredicted class probabilities for instance N-3: \",class_prob_dict_3)\n",
    "print(\"Predicted class for instance N-3: \", probs_3[1])\n",
    "print(\"\\nPredicted class probabilities for instance N-2: \",class_prob_dict_2 )\n",
    "print(\"Predicted class for instance N-2: \", probs_2[1])\n",
    "print(\"\\nPredicted class probabilities for instance N-1: \", class_prob_dict_1)\n",
    "print(\"Predicted class for instance N-1: \", probs_1[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell should act as your \"main\" function where you call the above functions \n",
    "# on the full Student data set, and print the evaluation score. [0.33 marks]\n",
    "\n",
    "\n",
    "\n",
    "# First, read in the data and apply your NB model to the Student data\n",
    "output = preprocess('student.csv')\n",
    "X = output[0]\n",
    "# After training the dataset, the label is always added to X, so create a copy using np\n",
    "label = output[1]\n",
    "X_and_label = output[3]\n",
    "train_result = train(X,label,X_and_label)\n",
    "label_prob_dict = train_result[0]\n",
    "\n",
    "feature_probs = train_result[1]\n",
    "score = evaluate(X,label,label_prob_dict,feature_probs)\n",
    "\n",
    "probs_3 = predict(X[-3],label_prob_dict,feature_probs)\n",
    "# round the number of decimals in the \"class probabilities\" dictionary\n",
    "class_prob_dict_3 = probs_3[0]\n",
    "for key_3 in class_prob_dict_3:\n",
    "    class_prob_dict_3[key_3] = round(class_prob_dict_3[key_3], 4)\n",
    "\n",
    "probs_2 = predict(X[-2],label_prob_dict,feature_probs)\n",
    "# round the number of decimals in the \"class probabilities\" dictionary\n",
    "class_prob_dict_2 = probs_2[0]\n",
    "for key_2 in class_prob_dict_2:\n",
    "    class_prob_dict_2[key_2] = round(class_prob_dict_2[key_2], 4)\n",
    "\n",
    "probs_1 = predict(X[-1],label_prob_dict,feature_probs)\n",
    "# round the number of decimals in the \"class probabilities\" dictionary\n",
    "class_prob_dict_1 = probs_1[0]\n",
    "for key_1 in class_prob_dict_1:\n",
    "    class_prob_dict_1[key_1] = round(class_prob_dict_1[key_1], 4)\n",
    "\n",
    "\n",
    "print(score)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Third, print data statistics and model predictions, as instructed below \n",
    "# N is the total number of instances, F the total number of features, L the total number of labels\n",
    "# The \"class probabilities\" may be unnormalized\n",
    "\n",
    "\n",
    "print(\"Feature vectors of instances [0, 1, 2]: \", X[0:3] )\n",
    "\n",
    "print(\"\\nNumber of instances (N): \", len(X))\n",
    "print(\"Number of features (F): \", len(X[0]))\n",
    "print(\"Number of labels (L): \", len(set(label)))\n",
    "\n",
    "print(\"\\n\\nPredicted class probabilities for instance N-3: \", class_prob_dict_3)\n",
    "print(\"Predicted class for instance N-3: \", probs_3[1])\n",
    "print(\"\\nPredicted class probabilities for instance N-2: \", class_prob_dict_2)\n",
    "print(\"Predicted class for instance N-2: \", probs_2[1])\n",
    "print(\"\\nPredicted class probabilities for instance N-1: \", class_prob_dict_1)\n",
    "print(\"Predicted class for instance N-1: \", probs_1[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obesity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This cell should act as your \"main\" function where you call the above functions \n",
    "# on the full Obesity data set, and print the evaluation score. [0.33 marks]\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# First, read in the data and apply your NB model to the Obesity data\n",
    "output = preprocess('obesity.csv')\n",
    "X = output[0]\n",
    "# After training the dataset, the label is always added to X, so create a copy using np\n",
    "label = output[1]\n",
    "X_and_label = output[3]\n",
    "train_result = train(X,label,X_and_label)\n",
    "label_prob_dict = train_result[0]\n",
    "feature_probs = train_result[1]\n",
    "score = evaluate(X,label,label_prob_dict,feature_probs)\n",
    "\n",
    "probs_3 = predict(X[-3],label_prob_dict,feature_probs)\n",
    "# round the number of decimals in the \"class probabilities\" dictionary\n",
    "class_prob_dict_3 = probs_3[0]\n",
    "for key_3 in class_prob_dict_3:\n",
    "    class_prob_dict_3[key_3] = round(class_prob_dict_3[key_3], 4)\n",
    "\n",
    "probs_2 = predict(X[-2],label_prob_dict,feature_probs)\n",
    "# round the number of decimals in the \"class probabilities\" dictionary\n",
    "class_prob_dict_2 = probs_2[0]\n",
    "for key_2 in class_prob_dict_2:\n",
    "    class_prob_dict_2[key_2] = round(class_prob_dict_2[key_2], 4)\n",
    "\n",
    "probs_1 = predict(X[-1],label_prob_dict,feature_probs)\n",
    "# round the number of decimals in the \"class probabilities\" dictionary\n",
    "class_prob_dict_1 = probs_1[0]\n",
    "for key_1 in class_prob_dict_1:\n",
    "    class_prob_dict_1[key_1] = round(class_prob_dict_1[key_1], 4)\n",
    "\n",
    "\n",
    "\n",
    "print(score)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Third, print data statistics and model predictions, as instructed below \n",
    "# N is the total number of instances, F the total number of features, L the total number of labels\n",
    "# The \"class probabilities\" may be unnormalized\n",
    "\n",
    "\n",
    "print(\"Feature vectors of instances [0, 1, 2]: \", X[0:3])\n",
    "\n",
    "print(\"\\nNumber of instances (N): \", len(X))\n",
    "print(\"Number of features (F): \", len(X[0]))\n",
    "print(\"Number of labels (L): \",len(set(label)) )\n",
    "\n",
    "print(\"\\n\\nPredicted class probabilities for instance N-3: \", class_prob_dict_3)\n",
    "print(\"Predicted class for instance N-3: \", probs_3[1])\n",
    "print(\"\\nPredicted class probabilities for instance N-2: \",class_prob_dict_2 )\n",
    "print(\"Predicted class for instance N-2: \", probs_2[1])\n",
    "print(\"\\nPredicted class probabilities for instance N-1: \", class_prob_dict_1)\n",
    "print(\"Predicted class for instance N-1: \", probs_1[1])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: Conceptual questions [13 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: One-R Baseline [3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write additional code here, if necessary (you may insert additional code cells)\n",
    "# You should implement the One-R classifier from scratch. Do not use existing implementations/learning algorithms.\n",
    "# Print the feature name and its corresponding error rate that One-R selects, in addition to any evaluation scores.\n",
    "\n",
    "# implement baseline model on student and one other dataset\n",
    "# the second input should be the feature vector of all instances\n",
    "# the third input is the label\n",
    "from sklearn.metrics import f1_score\n",
    "def baseline_classifier(column_names,X,label):\n",
    "    num_features = len(X[0])\n",
    "    poss_feat_val = list()\n",
    "    possible_labels = list(set(label))\n",
    "    # create a copy of each entry in X appended with its corresponding label\n",
    "    # the denominator to calculate the error rate is the length of the label list\n",
    "    denominator = len(label)\n",
    "    search_space = list()\n",
    "    for m in range(len(X)):\n",
    "        temp = X[m]\n",
    "        temp.append(label[m])\n",
    "        search_space.append(temp)\n",
    "    search_space = np.array(search_space)\n",
    "    for i in range(num_features):\n",
    "        temp = list(set([instance[i] for instance in X]))\n",
    "        poss_feat_val.append(temp)\n",
    "    # initialise a list to remember all labels of all feature values labels\n",
    "    all_feat_lab = list()\n",
    "    # initialise a list to remember all error rates\n",
    "    err_rates = list()\n",
    "    # go through each feature-value set\n",
    "    for j in range(len(poss_feat_val)):\n",
    "        # go through each value in the feature\n",
    "        running_error = 0\n",
    "        my_temp = list()\n",
    "        for k in range(len(poss_feat_val[j])):\n",
    "            val_label_count_dict = dict()\n",
    "            # go through each available label\n",
    "            for indi_lab in possible_labels:\n",
    "               result = search_space[np.where( (search_space[:,j] == poss_feat_val[j][k]) & (search_space[:,-1] == indi_lab) )]\n",
    "               register_key = poss_feat_val[j][k] + \"_\" + indi_lab\n",
    "               val_label_count_dict[register_key] = len(result)\n",
    "            # find the maximum key in the in the val_label_count_dict\n",
    "            max_key = max(val_label_count_dict, key = val_label_count_dict.get)\n",
    "            underscore_index = max_key.index(\"_\") \n",
    "            feat_val = max_key[:underscore_index] \n",
    "            max_label = max_key[underscore_index + 1: ] \n",
    "            # need to save the max label for each feature value\n",
    "            my_temp.append({feat_val:max_label})\n",
    "            # remove the max key from the dictionary\n",
    "            val_label_count_dict.pop(max_key)\n",
    "            # get all other values from the dictionary\n",
    "            values = val_label_count_dict.values()\n",
    "            total = sum(values) \n",
    "            running_error += total\n",
    "        running_error /= denominator\n",
    "        err_rates.append(running_error)\n",
    "        all_feat_lab.append(my_temp) \n",
    "\n",
    "    # after all the loops are completed, find the minimum error rate\n",
    "    min_error = min(err_rates)   \n",
    "    # find the index of the min err_rates\n",
    "    index_min_err = err_rates.index(min_error)\n",
    "    # find the corresponding array with the class labels\n",
    "    one_r_feat = all_feat_lab[index_min_err]\n",
    "    best_feature = column_names[index_min_err]\n",
    "    return (best_feature,min_error,one_r_feat, index_min_err)\n",
    "\n",
    "# write a small evaluation function\n",
    "def evaluate_2(X,label,one_r_feat,index_min_err):\n",
    "    best_feat_test = [instance[index_min_err] for instance in X]\n",
    "    predictions = list()\n",
    "    one_r_res = dict()\n",
    "    for entry in one_r_feat:\n",
    "        one_r_res.update(entry)\n",
    "    # generate the predicition for each instance\n",
    "    for i in range(len(best_feat_test)):\n",
    "        feature_value = best_feat_test[i]\n",
    "        temp = one_r_res[feature_value]\n",
    "        predictions.append(temp)\n",
    "    # count the number of labels\n",
    "    number_of_labels = len(list(set(label)))\n",
    "    label_types = list(set(label))\n",
    "    evaluation_scores = dict()\n",
    "    if(number_of_labels == 2):\n",
    "        evaluation_scores['f1_score'] = f1_score(label,predictions,pos_label=label_types[0],average='binary')\n",
    "        evaluation_scores['precision'] = precision_score(label,predictions,pos_label=label_types[0],average='binary',zero_division=0)\n",
    "        evaluation_scores['recall'] = recall_score(label,predictions,pos_label=label_types[0],average='binary',zero_division=0)\n",
    "    elif(number_of_labels > 2):\n",
    "        evaluation_scores['f1_score'] = f1_score(label,predictions,average='macro')\n",
    "        evaluation_scores['precision'] = precision_score(label,predictions,average='macro',zero_division=0)\n",
    "        evaluation_scores['recall'] = recall_score(label,predictions,average='macro',zero_division=0)\n",
    "\n",
    "    evaluation_scores['accuracy'] = accuracy_score(label,predictions)\n",
    "    evaluation_scores['error_rate'] = 1 - evaluation_scores['accuracy']\n",
    "    return evaluation_scores\n",
    "\n",
    "# before implementing baseline_classifier, need to implement preprocess function first\n",
    "# implement the baseline model on student dataset\n",
    "# get the F1 score macro-average\n",
    "print('Student: ')\n",
    "output = preprocess('student.csv')\n",
    "X = output[0]\n",
    "label = output[1]\n",
    "column_names = output[2]\n",
    "result = baseline_classifier(column_names,X,label)\n",
    "print(\"The feature name that one-R selects is: \",result[0],\". The corresponding error rate: \", round(result[1],4))\n",
    "score_1 = evaluate_2(X,label,result[2],result[3])\n",
    "print(score_1)\n",
    "\n",
    "\n",
    "print('\\nObesity')\n",
    "output_2 = preprocess('obesity.csv')\n",
    "X_2 = output_2[0]\n",
    "label_2 = output_2[1]\n",
    "column_names_2 = output_2[2]\n",
    "result_2 = baseline_classifier(column_names_2,X_2,label_2)\n",
    "score_3 = evaluate_2(X_2,label_2,result_2[2],result_2[3])\n",
    "print(\"The feature name that one-R model selects is: \", result_2[0],\". The corresponding error rate: \",round(result_2[1],4))\n",
    "print(score_3)\n",
    "\n",
    "print('\\nBank Marketing')\n",
    "output_2 = preprocess('bank-marketing.csv')\n",
    "X_2 = output_2[0]\n",
    "label_2 = output_2[1]\n",
    "column_names_2 = output_2[2]\n",
    "result_2 = baseline_classifier(column_names_2,X_2,label_2)\n",
    "score_3 = evaluate_2(X_2,label_2,result_2[2],result_2[3])\n",
    "print(\"The feature name that one-R model selects is: \", result_2[0],\". The corresponding error rate: \",round(result_2[1],4))\n",
    "print(score_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1.b </br>\n",
    "\n",
    "In the Student Dataset, the error rate of the one-r baseline model was much higher as this model had a rule of majority voting of labels for each attribute value (low,none,mid,high) of the feature Fedu. This is justified by the accuracy of the Naive Bayes model which was 0.4848 (higher than 0.3344) which means more total true positives,recall which was 0.4763 (higher than 0.1896) which means less false negative,precision which was 0.4835(higher than 0.1067) which menas less false positive and f1_score which was 0.4788 (higher than 0.1276), error rate of 0.5161 (lower than 0.6656). The same applies for the Obesity dataset, with the accuracy of its Naive Bayes model being 0.7783 (higher than 0.6652), precision of 0.71 (higher than 0.5585), error rate of 0.221(lower than 0.3647) however the recall which was 0.8765 is lower than the baseline model of (0.9917), suggesting there are more false negative results in the Naive Bayes model. Nonetheless, since most performance parameters of the Naive Bayes model in obesity dataset exceeds the one-line model, we can say the Naive Bayes classifier outperforms the baseline model.\n",
    "\n",
    "This is because the one-r baseline model lacks complexity and have little predictive power. In the Naive Bayes model, all probabilities of attributes values given the labels was calculated and log aggregated to find the best label whereas only one or the best label corresponding to the attribute value in which its attribute has the lowest error rate, is used in the one-r baseline model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Evaluation strategy [3 marks] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write additional code here, if necessary (you may insert additional code cells)\n",
    "# cross validation\n",
    "from itertools import islice\n",
    "import math\n",
    "import statistics\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "def division_sets(X,label):\n",
    "    no_blocks = 10\n",
    "    length_to_split = list()\n",
    "    block_size = math.floor(len(X) / 10)\n",
    "    residue_size = len(X) % block_size\n",
    "    if residue_size == 0:\n",
    "        length_to_split = [block_size] * 10\n",
    "    else:\n",
    "        length_to_split = [block_size] * 9\n",
    "        length_to_split.append(block_size+residue_size)\n",
    "    X_copy = iter(X)\n",
    "    label_copy = iter(label)\n",
    "    sliced_instances = [list(islice(X_copy,elem)) for elem in length_to_split]\n",
    "    sliced_label = [list(islice(label_copy,elem)) for elem in length_to_split]\n",
    "    return (sliced_instances,sliced_label)\n",
    "\n",
    "def join_train_label(sliced_instances,sliced_label):\n",
    "    instance_and_label = list()\n",
    "    for i in range(len(sliced_instances)):\n",
    "        temp = list(sliced_instances[i])\n",
    "        temp.append(sliced_label[i])\n",
    "        instance_and_label.append(temp)\n",
    "    return instance_and_label\n",
    "def cross_validation(division_sets,join_train_and_label, X, label):\n",
    "    sliced_element = division_sets(X,label)\n",
    "    label_copy = list(label)\n",
    "    number_of_labels = len(list(set(label)))\n",
    "    label_types = list(set(label))\n",
    "    instance = sliced_element[0]\n",
    "    label = sliced_element[1]\n",
    "    fones = list()\n",
    "    precisions = list()\n",
    "    accuracies = list()\n",
    "    recalls = list()\n",
    "    errs = list()\n",
    "    accuracies = list()\n",
    "    for i in range(10):\n",
    "        test_set = instance[i]\n",
    "        test_label = label[i]\n",
    "        train_set = list()\n",
    "        train_label = list()\n",
    "        if i > 0:\n",
    "            for m in range(0,i):\n",
    "                train_set += instance[m]\n",
    "                train_label += label[m]\n",
    "                \n",
    "        for j in range(i+1,10):\n",
    "            train_set += instance[j]\n",
    "            train_label += label[j]\n",
    "        train_set_and_label = join_train_and_label(train_set,train_label)\n",
    "        param = train(train_set,train_label,train_set_and_label)\n",
    "        predictions = list()\n",
    "        for k in range(len(test_set)):\n",
    "            current_pred = predict(test_set[k],param[0],param[1])\n",
    "            predictions.append(current_pred[1])\n",
    "        # calculate the number of label types\n",
    "       \n",
    "        if (number_of_labels == 2):\n",
    "            # remember to append as well\n",
    "            f1 = f1_score(test_label,predictions,pos_label=label_types[0],average='binary')\n",
    "            precision = precision_score(test_label,predictions,pos_label=label_types[0],average='binary',zero_division=0)\n",
    "            recall = recall_score(test_label,predictions,pos_label=label_types[0],average='binary',zero_division=0)\n",
    "            fones.append(f1)\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "        elif(number_of_labels > 2):\n",
    "            f1 = f1_score(test_label,predictions,average='macro')\n",
    "            precision = precision_score(test_label,predictions,average='macro',zero_division=0)\n",
    "            recall = recall_score(test_label,predictions,average='macro',zero_division=0)\n",
    "            fones.append(f1)\n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "        accuracy = accuracy_score(test_label,predictions)\n",
    "        accuracies.append(accuracy)\n",
    "        err = 1 - accuracy_score(test_label,predictions)\n",
    "        errs.append(err)\n",
    "    evaluation_scores = dict()\n",
    "    evaluation_scores['precision'] = statistics.mean(precisions)\n",
    "    evaluation_scores['recall'] = statistics.mean(recalls)\n",
    "    evaluation_scores['f1_score'] = statistics.mean(fones)\n",
    "    evaluation_scores['accuracy_score'] = statistics.mean(accuracies)\n",
    "    evaluation_scores['error_rate'] = statistics.mean(errs)\n",
    "    return evaluation_scores\n",
    "output_1 = preprocess('bank-marketing.csv')\n",
    "X_1 = output_1[0]\n",
    "label_1 = output_1[1]\n",
    "result = cross_validation(division_sets,join_train_label,X_1,label_1)\n",
    "\n",
    "print('Bank marketing')\n",
    "print(result)\n",
    "\n",
    "\n",
    "\n",
    "# To be used for comparison in Question 4\n",
    "output_3 = preprocess('student.csv')\n",
    "X_3 = output_3[0]\n",
    "label_3 = output_3[1]\n",
    "result_3 = cross_validation(division_sets,join_train_label,X_3,label_3)\n",
    "print('\\nStudent')\n",
    "print(result_3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2 </br>\n",
    "In the bank marketing dataset, the training error (performing test on training data) (0.1126) is slightly lower than the averaged cross-validation error (0.1137). The precision of cross-validation 0.5166 is much lower than 0.9077 when performing test on training data, suggesting there was a much higher proportion of false positive results in averaged cross-validation. The recall of value of 0.2397 is lower than 0.9715, meaning that the proportion of false negative results is much higher in the averaged cross-validation. There is only a slight difference of accuracy score (0.887 in test on training data and 0.886 in in averaged cross-validation). This is because the bank marketing dataset is imbalanced and most of the instances had the label 'no'. The same justification is applied to the slight difference in error rates as error rate is complementary to accuracy. \n",
    "\n",
    "On the other hand, the training error on the Student dataset (0.5161) is much lower than the averaged cross-validation error of 0.6406. The recall of cross validation is 0.2864, which is lower than 0.4763 when testing on training data. The precision  in cross-validation is 0.29 which is lower than 0.4835. \n",
    "\n",
    "In conclusion, both the bank marketing and student dataset exhibits lower performance on  cross-validation compared to testing on training data. This is because testing on training data provides a gross overestimation of the Naive Bayes classifier performance. This is because we are effectively telling the classifier what the answers are, while aking the classifier to provide the correct answer. Although all instances acted as both training and testing dataset in cross-validation as well, but the training and testing instances did not overlap in each iteration (fold) of calculating averaged cross-validation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Feature Selection and Naive Bayes Assumptions [3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write additional code here, if necessary (you may insert additional code cells)\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "import matplotlib.pyplot as plt\n",
    "# Mutual Information Obesity and Student Dataset\n",
    "df = pd.read_csv('obesity.csv')\n",
    "X = df.iloc[:,1:-1]\n",
    "x1 = X.apply(preprocessing.LabelEncoder().fit_transform)\n",
    "y = df.iloc[:, -1]\n",
    "scores = mutual_info_classif(x1,y,discrete_features=True)\n",
    "features = df.columns[1:-1]\n",
    "\n",
    "plt.barh(features,scores)\n",
    "for index, value in enumerate(scores):\n",
    "    plt.text(value,index,str(round(value,4)))\n",
    "plt.ylabel(\"Obesity features\")\n",
    "plt.xlabel(\"MI scores\")\n",
    "plt.title(\"Obesity features with scores\")\n",
    "plt.show()\n",
    "\n",
    "student_data = pd.read_csv('student.csv')\n",
    "student_X = student_data.iloc[:,1:-1]\n",
    "student_x1 = student_X.apply(preprocessing.LabelEncoder().fit_transform)\n",
    "student_y = student_data.iloc[:,-1]\n",
    "student_scores = mutual_info_classif(student_x1,student_y,discrete_features=True)\n",
    "\n",
    "student_features = student_data.columns[1:-1]\n",
    "plt.barh(student_features,student_scores)\n",
    "for index,value in enumerate(student_scores):\n",
    "    plt.text(value,index,str(round(value,4)))\n",
    "plt.ylabel('Student features')\n",
    "plt.xlabel('MI scores')\n",
    "plt.title('Student features with scores')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr\n",
    "obesity_X_list = X.values.tolist()\n",
    "obesity_y_list = y.values.tolist()\n",
    "obesity_precisions = list()\n",
    "obesity_recalls = list()\n",
    "obesity_fones = list()\n",
    "obesity_accuracies = list()\n",
    "\n",
    "for m in range(len(obesity_X_list[0])):\n",
    "    feature = [[instance [m]] for instance in obesity_X_list]\n",
    "    evaluation_scores = cross_validation(division_sets,join_train_label,feature,obesity_y_list)\n",
    "    obesity_precisions.append(evaluation_scores['precision'])\n",
    "    obesity_recalls.append(evaluation_scores['recall'])\n",
    "    obesity_fones.append(evaluation_scores['f1_score'])\n",
    "    obesity_accuracies.append(evaluation_scores['accuracy_score'])\n",
    "\n",
    "student_X_list = student_X.values.tolist()\n",
    "student_y_list = student_y.values.tolist()\n",
    "\n",
    "student_precisions = list()\n",
    "student_recalls = list()\n",
    "student_fones = list()\n",
    "student_accuracies = list()\n",
    "for n in range(len(student_X_list[0])):\n",
    "     feature = [[instance [n]] for instance in student_X_list]\n",
    "     student_evaluation = cross_validation(division_sets,join_train_label,feature,student_y_list)\n",
    "     student_precisions.append(student_evaluation['precision'])\n",
    "     student_recalls.append(student_evaluation['recall'])\n",
    "     student_fones.append(student_evaluation['f1_score'])\n",
    "     student_accuracies.append(student_evaluation['accuracy_score'])\n",
    "\n",
    "# see how well MI scores correlate to performance\n",
    "print('Obesity Dataset: ')\n",
    "plt.figure(0)\n",
    "plt.scatter(scores,obesity_accuracies)\n",
    "plt.xlabel('MI Scores')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs MI Scores')\n",
    "corrMI_acc = pearsonr(scores,obesity_accuracies)\n",
    "print('The correlation between MI scores and accuracy is: ', round(corrMI_acc[0],4))\n",
    "corrMI_pre = pearsonr(scores,obesity_precisions)\n",
    "print('The correlation between MI scores and precision is: ',round(corrMI_pre[0],4))\n",
    "corrMI_rec = pearsonr(scores,obesity_recalls)\n",
    "print('The correlation between MI scores and recall is: ', round(corrMI_rec[0],4))\n",
    "corrMI_f1 = pearsonr(scores,obesity_fones)\n",
    "print('The correlation between MI scores and f_1 is: ', round(corrMI_f1[0],4))\n",
    "plt.figure(1)\n",
    "plt.scatter(scores,obesity_precisions)\n",
    "plt.xlabel('MI Scores')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision vs MI Scores')\n",
    "plt.figure(2)\n",
    "plt.scatter(scores,obesity_recalls)\n",
    "plt.xlabel('MI Scores')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Recall vs MI Scores')\n",
    "plt.figure(3)\n",
    "plt.scatter(scores,obesity_fones)\n",
    "plt.xlabel('MI Scores')\n",
    "plt.ylabel('F1_score')\n",
    "plt.title('F1_score vs MI Scores')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Student Dataset: ')\n",
    "plt.figure(0)\n",
    "plt.scatter(student_scores,student_accuracies)\n",
    "plt.xlabel('MI scores')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy vs MI scores')\n",
    "corrMI_acc = pearsonr(student_scores,student_accuracies)\n",
    "print('The correlation between MI scores and accuracy is: ', round(corrMI_acc[0],1),'.The p-value is: ', round(corrMI_acc[1], 4))\n",
    "\n",
    "plt.figure(1)\n",
    "plt.scatter(student_scores,student_precisions)\n",
    "plt.xlabel('MI scores')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision vs MI scores')\n",
    "corrMI_prec = pearsonr(student_scores,student_precisions)\n",
    "print('The correlation between MI scores and precision is: ', round(corrMI_prec[0],4),'.The p-value is: ', round(corrMI_prec[1], 4))\n",
    "\n",
    "\n",
    "plt.figure(2)\n",
    "plt.scatter(student_scores,student_recalls)\n",
    "plt.xlabel('MI scores')\n",
    "plt.ylabel('Recall')\n",
    "plt.title('Recall vs MI scores')\n",
    "corrMI_rec = pearsonr(student_scores,student_recalls)\n",
    "print('The correlation between MI scores and recall is: ', round(corrMI_rec[0],4),'.The p-value is: ', round(corrMI_rec[1], 4))\n",
    "\n",
    "plt.figure(3)\n",
    "plt.scatter(student_scores,student_fones)\n",
    "plt.xlabel('MI scores')\n",
    "plt.ylabel('F1_score')\n",
    "plt.title('F1_score vs MI scores')\n",
    "corrMI_f1 = pearsonr(student_scores,student_fones)\n",
    "print('The correlation between MI scores and F1_score is: ', round(corrMI_f1[0],4),'.The p-value is: ', round(corrMI_f1[1],4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3.a </br>\n",
    "In the Obesity dataset, family_history_with_overweight has the highest MI score and hence highest utility(MI: 0.1105) for the classification task, followed by consumption of food between meals(CAEC) (MI: 0.0819) and then number of main meals (NCP) (0.0402). Some features in the Obesity dataset provides minimal utility to the classification task such as smoke (MI:0.0001) and gender (MI :0). We can say that the MI scores in the Obesity dataset is positively correlated with the utlity/ performance of the classifier because: MI scores has a strong positively correlated relationship with the accuracy (correlation: 0.8578). The same relationship is the same for other evaluation parameters such as recall (correlation: 0.7773) and f1(0.6782). MI score is only moderately postive correlated with precision (correlationL 0.486). family_history_with_overweight and CAEC results in highest utlity in the classification task because the occurrence of these attribute values is highly correlated with the categorical labels that appear in the training dataset. On the other hand, MTRANS and gender are redundant to the dataset, meaning the occurrence of their feature values is completely random and does not correlate with the categorical labels that appear in the dataset. \n",
    "\n",
    "In the Student dataset, failures have the highest MI score (MI: 0.115), followed by higher (0.0705)  and Medu (MI: 0.0619) Some features in the Student dataset has lower MI score, such as Pstatus(MI: 0.001), famsize (MI: 0.0022) and sex (MI: 0.0098). We noticed that there is a weak negative relationship between MI scores and accuracy, however since the p-value for this coefficient is greater than 0.05, we can ignore this performance parameter. On the other hand, MI scores have a moderately strong positive relationship with precision (correlatiom: 0.5691), recall(0.5404) and F1-score (0.5105). Hence, this means failure has the highest utility for the model, followed by higher and Medu, whereas famsize has the lowest utility for the model, followed by sex and Pstatus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3.b </br>\n",
    "(1) When there are many attributes in the dataset (29 for Student and 13 for Obesity), it would be computationally expensive to calculate P(x1,x2,...,xn | y). To reduce the computation of evaluating this probability, the conditional independence assumption , which is the naivety of the Naive Bayes model is made. This means that conditioned on the class y, the features are assumed to be independent. Also, instances are independent of each other. Stratification should apply in the naivety assumption, meaning the testing data should have the same distribution as the training data.\n",
    "\n",
    "(2) In the Obesity dataset, the Naive Bayes assumption tells us that given each class (A+, A, B, C, D, F), we treat all features as independent. However, the frequency of consumption of high caloric food (FAVC) is dependent on family_history_of_overweight. This is because the diet habits of an individual is highly influenced by the diet choices of their family. Those who prefer to walk or bike will likely have higher FAF. In the Student dataset, Naive Bayes assumption may be problematic as failures is dependent on the extra educational support provided by the family and the school(famsup & schoolsup).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Feature Selection and Ethics [4 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write additional code here, if necessary (you may insert additional code cells)\n",
    "import pandas as pd\n",
    "student_data = pd.read_csv('student.csv')\n",
    "student_data.head()\n",
    "student_copy = student_data.drop(labels=['address','Pstatus','Mjob','Fjob','famrel','freetime','internet','health','romantic'],axis=1)\n",
    "data_list = student_copy.values.tolist()\n",
    "X = [instance[1:-1] for instance in data_list]\n",
    "label = [instance[-1] for instance in data_list]\n",
    "ethical_result = cross_validation(division_sets,join_train_label,X,label)\n",
    "print('Student Ethical Result Processing')\n",
    "print(ethical_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4.a </br>\n",
    "One ethical problem that may arise is social bias. The attributes address (urban or rural), MJob (mother's job) and Fjob (father's job) and internet (internet access at home) is highly correlated with socioeconomic status. The classifier may unintentionally be biased against students from low socioeconomic backgrounds. Also, Pstatus (parents cohabitation status) and famrel is related to marital status. Other than that, students may be biased based on their health conditions. Students will also be judged based on their involvement in a romantic relationship, which are all unfair indicators when predicting college grades. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4.b</br>\n",
    "There is an general increase in performance compared to when using the full classifier (3 out of 4 performance parameters increased, error rate not considered since it is complementary with accuracy). Using cross-validation evaluation on Student dataset after removing all senstive attributes, the precision increased from 0.29 to 0.30, the recall increased from 0.2864 to 0.2948, the f1_score increased from 0.2657 to 0.2702, the accuracy score decreased from 0.3594 to 0.3514, the error rate increased from 0.6405 to 0.6486. This performance change occurs because the resulting data set carry less biases of the people labelling the data, meaning that the training data became better, which resulted in a better predicition.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4.c </br>\n",
    "Removing all features in (b) does not guarantee a fair classifier. This is because other variables in the student dataset such as Medu (mother's education), Fedu (father's education), higher (wants to take higher education) may still be highly correlated with the removed attributes. For example, the students decision of continuing with higher education may be affected by the family's available funds to support the student's college tuition instead of their interest in academic persuit.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Authorship Declaration</b>:\n",
    "\n",
    "   (1) I certify that the program contained in this submission is completely\n",
    "   my own individual work, except where explicitly noted by comments that\n",
    "   provide details otherwise.  I understand that work that has been developed\n",
    "   by another student, or by me in collaboration with other students,\n",
    "   or by non-students as a result of request, solicitation, or payment,\n",
    "   may not be submitted for assessment in this subject.  I understand that\n",
    "   submitting for assessment work developed by or in collaboration with\n",
    "   other students or non-students constitutes Academic Misconduct, and\n",
    "   may be penalized by mark deductions, or by other penalties determined\n",
    "   via the University of Melbourne Academic Honesty Policy, as described\n",
    "   at https://academicintegrity.unimelb.edu.au.\n",
    "\n",
    "   (2) I also certify that I have not provided a copy of this work in either\n",
    "   softcopy or hardcopy or any other form to any other student, and nor will\n",
    "   I do so until after the marks are released. I understand that providing\n",
    "   my work to other students, regardless of my intention or any undertakings\n",
    "   made to me by that other student, is also Academic Misconduct.\n",
    "\n",
    "   (3) I further understand that providing a copy of the assignment\n",
    "   specification to any form of code authoring or assignment tutoring\n",
    "   service, or drawing the attention of others to such services and code\n",
    "   that may have been made available via such a service, may be regarded\n",
    "   as Student General Misconduct (interfering with the teaching activities\n",
    "   of the University and/or inciting others to commit Academic Misconduct).\n",
    "   I understand that an allegation of Student General Misconduct may arise\n",
    "   regardless of whether or not I personally make use of such solutions\n",
    "   or sought benefit from such actions.\n",
    "\n",
    "   <b>Signed by</b>: Cheah Jia Huei 1078203\n",
    "   \n",
    "   <b>Dated</b>: 29/08/2022"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "bb600fda251cf4f2b63845bfedaf16cb92e60656290d508957de2e5b2bb243fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
